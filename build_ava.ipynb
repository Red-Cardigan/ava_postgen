{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build on original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
    "import time\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "import psycopg2\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "from psycopg2 import pool\n",
    "import re\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'host': \"localhost\",\n",
    "    'database': \"postgres\",\n",
    "    'user': \"cardigan\"\n",
    "}\n",
    "\n",
    "# Initialize connection pool\n",
    "conn_pool = psycopg2.pool.SimpleConnectionPool(1, 10, **db_params)\n",
    "\n",
    "# Initialize UserAgent object\n",
    "ua = UserAgent()\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "base_url = \"https://substack.com/api/v1/category/public/{id}/all?page={page}\"\n",
    "\n",
    "ID_list = [96, 4, 15417, 134, 114, 18, 284]\n",
    "#culture, technology, art, science, philosophy, history, fiction\n",
    "\n",
    "# https://substack.com/api/v1/category/public/284/all?page=0\n",
    "\n",
    "def get_headers():\n",
    "    headers = {\n",
    "        \"User-Agent\": ua.random,\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "def random_sleep():\n",
    "    time.sleep(random.uniform(0.5, 1))\n",
    "\n",
    "def parse_subscriber_count(subscriber_text):\n",
    "    subscriber_text = subscriber_text.replace('subscribers', '').strip()\n",
    "    match = re.match(r'(\\d+)([KkMm])?\\+?', subscriber_text)\n",
    "    if match:\n",
    "        number, multiplier = match.groups()\n",
    "        number = int(number)\n",
    "        if multiplier:\n",
    "            if multiplier.lower() == 'k':\n",
    "                number *= 1000\n",
    "            elif multiplier.lower() == 'm':\n",
    "                number *= 1000000\n",
    "        return number\n",
    "    else:\n",
    "        return 10\n",
    "\n",
    "def get_subscriber_count(handle_url):\n",
    "    subscriber_count = 10\n",
    "    if handle_url:\n",
    "        retries = 0\n",
    "        while retries < MAX_RETRIES:\n",
    "            try:\n",
    "                # Update the headers of your requests\n",
    "                headers = get_headers()\n",
    "                response = requests.get(handle_url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                subscribers_found = False\n",
    "                for a_tag in soup.find_all('a'):\n",
    "                    if 'subscribers' in a_tag.text:\n",
    "                        subscribers_found = True\n",
    "                        subscriber_count = parse_subscriber_count(a_tag.text)\n",
    "                        break\n",
    "\n",
    "                if not subscribers_found:\n",
    "                    logging.info(f\"No 'subscribers' found for {handle_url}\")\n",
    "                    subscriber_count = 10\n",
    "\n",
    "                break  # Exit the loop early if 'subscribers' is not found\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                if e.response is not None and e.response.status_code == 429:  # Rate limit error\n",
    "                    logging.warning(f\"Rate limit reached at {handle_url}. Retrying in {2 ** retries} seconds.\")\n",
    "                    time.sleep(min(2 ** retries, 10))  # Exponential backoff, max 60 seconds\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    logging.warning(f\"Error fetching {handle_url}: {e}. Skipping to next URL.\")\n",
    "                    break  # Skip to the next URL on any error other than rate limit\n",
    "    return subscriber_count\n",
    "\n",
    "def append_to_db(data_list):\n",
    "    conn = conn_pool.getconn()\n",
    "    with conn.cursor() as cur:\n",
    "        for data in data_list:\n",
    "            cur.execute(\n",
    "                \"SELECT 1 FROM writers WHERE handle_url = %(handle)s\",\n",
    "                {\"handle\": data[\"handle\"]}\n",
    "            )\n",
    "            if cur.fetchone() is None:\n",
    "                cur.execute(\n",
    "                    \"INSERT INTO writers (id, handle_url, subscribers, subdomain_url) VALUES (%(id)s, %(handle)s, %(subscribers)s, %(subdomain)s)\",\n",
    "                    data\n",
    "                )\n",
    "    conn.commit()\n",
    "    conn_pool.putconn(conn)\n",
    "\n",
    "headers = get_headers()\n",
    "s = requests.Session()\n",
    "s.headers.update(headers)\n",
    "\n",
    "for ID in ID_list:\n",
    "    for page in range(21):\n",
    "        url = base_url.format(id=ID, page=page)\n",
    "        attempt = 0\n",
    "        max_attempts = 5\n",
    "        backoff_time = 1\n",
    "\n",
    "        while attempt < max_attempts:\n",
    "            try:\n",
    "                response = s.get(url)\n",
    "                if response.status_code == 429:  # Check for rate limit error\n",
    "                    print(\"Rate limit reached. Sleeping...\")\n",
    "                    time.sleep(backoff_time)  # Sleep if rate limit is reached\n",
    "                    backoff_time *= 2  # Increase backoff time\n",
    "                    continue  # Skip the rest of the loop and try again\n",
    "\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                data_list = []\n",
    "                for publication in data.get('publications', []):\n",
    "                    user_id = publication.get('author_id')\n",
    "                    handle = publication.get('author_handle')\n",
    "                    subdomain = publication.get('subdomain')\n",
    "                    if user_id and handle and subdomain:\n",
    "                        handle_url = f\"https://substack.com/@{handle}\"\n",
    "                        subdomain_url = f\"https://{subdomain}.substack.com\"\n",
    "                        subscriber_count = get_subscriber_count(handle_url)\n",
    "                        if subscriber_count >= 100:\n",
    "                            data_list.append({\n",
    "                                \"handle\": handle_url,\n",
    "                                \"id\": user_id,\n",
    "                                \"subscribers\": subscriber_count,\n",
    "                                \"subdomain\": subdomain_url\n",
    "                            })\n",
    "                append_to_db(data_list)\n",
    "                print(f\"Stored responses for {ID}, page {page}\")\n",
    "                break\n",
    "            except (ConnectionError, Timeout, TooManyRedirects) as e:\n",
    "                print(f\"Error fetching data from {url}. Error: {e}\")\n",
    "\n",
    "            time.sleep(backoff_time)\n",
    "            backoff_time *= 2\n",
    "            attempt += 1\n",
    "\n",
    "        if attempt == max_attempts:\n",
    "            print(f\"Max attempts reached for URL: {url}\")\n",
    "\n",
    "# Close the connection pool\n",
    "conn_pool.closeall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds following/follower lists (redundant)\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "\n",
    "db_params = {\n",
    "    'host': \"localhost\",\n",
    "    'database': \"postgres\",\n",
    "    'user': \"cardigan\"\n",
    "}\n",
    "\n",
    "# Initialize connection pool\n",
    "conn_pool = psycopg2.pool.SimpleConnectionPool(1, 10, **db_params)\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Copy users_they_follow_set and users_following_them_set from other tables\n",
    "            cur.execute(\"\"\"\n",
    "                UPDATE writers w\n",
    "                SET\n",
    "                    users_following_them_set = subq.users_following_them,\n",
    "                    users_they_follow_set = subq.users_they_follow\n",
    "                FROM \n",
    "                    (SELECT \n",
    "                        coalesce(ca.\"User_ID\", c_a.user_id, wn1.id, wn2.id, wn3.id) as id,\n",
    "                        COALESCE(ca.users_following_them, c_a.users_following_them, wn1.users_following_them, wn2.users_following_them, wn3.users_following_them) as users_following_them,\n",
    "                        COALESCE(ca.users_they_follow, c_a.users_they_follow, wn1.users_they_follow, wn2.users_they_follow, wn3.users_they_follow) as users_they_follow\n",
    "                    FROM \n",
    "                        following_lists ca\n",
    "                        FULL OUTER JOIN core_augmented c_a ON ca.\"User_ID\" = c_a.user_id\n",
    "                        FULL OUTER JOIN well_networked_1 wn1 ON ca.\"User_ID\" = wn1.id\n",
    "                        FULL OUTER JOIN well_networked_2 wn2 ON ca.\"User_ID\" = wn2.id\n",
    "                        FULL OUTER JOIN well_networked_3 wn3 ON ca.\"User_ID\" = wn3.id\n",
    "                    ) AS subq\n",
    "                WHERE \n",
    "                    w.id = subq.id\n",
    "            \"\"\")\n",
    "            conn.commit()\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "    conn.rollback()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape number of subscribers (redundant, done in last cell)\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "from psycopg2 import pool\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "db_params = {\n",
    "    'host': \"localhost\",\n",
    "    'database': \"postgres\",\n",
    "    'user': \"cardigan\"\n",
    "}\n",
    "\n",
    "conn_pool = psycopg2.pool.SimpleConnectionPool(1, 10, **db_params)\n",
    "\n",
    "# Initialize UserAgent object\n",
    "ua = UserAgent()\n",
    "\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "def get_headers():\n",
    "    headers = {\n",
    "        \"User-Agent\": ua.random,\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "def random_sleep():\n",
    "    time.sleep(random.uniform(0.5, 1))\n",
    "\n",
    "def parse_subscriber_count(subscriber_text):\n",
    "    subscriber_text = subscriber_text.replace('subscribers', '').strip()\n",
    "    match = re.match(r'(\\d+)([KkMm])?\\+?', subscriber_text)\n",
    "    if match:\n",
    "        number, multiplier = match.groups()\n",
    "        number = int(number)\n",
    "        if multiplier:\n",
    "            if multiplier.lower() == 'k':\n",
    "                number *= 1000\n",
    "            elif multiplier.lower() == 'm':\n",
    "                number *= 1000000\n",
    "        return number\n",
    "    else:\n",
    "        return '10'\n",
    "\n",
    "def get_subscriber_count(handle_url, user_id, conn):\n",
    "    subscriber_count = '10'\n",
    "    if handle_url:\n",
    "        retries = 0\n",
    "        while retries < MAX_RETRIES:\n",
    "            try:\n",
    "                conn = conn_pool.getconn()\n",
    "                with conn.cursor() as cur:\n",
    "                    # Update the headers of your requests\n",
    "                    headers = get_headers()\n",
    "                    response = requests.get(handle_url, headers=headers)\n",
    "                    response.raise_for_status()\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    subscribers_found = False\n",
    "                    for a_tag in soup.find_all('a'):\n",
    "                        if 'subscribers' in a_tag.text:\n",
    "                            subscribers_found = True\n",
    "                            subscriber_count = parse_subscriber_count(a_tag.text)\n",
    "                            break\n",
    "\n",
    "                    if not subscribers_found:\n",
    "                        logging.info(f\"No 'subscribers' found for {handle_url}\")\n",
    "                        subscriber_count = '10'\n",
    "\n",
    "                try:\n",
    "                    with conn.cursor() as cur:\n",
    "                        cur.execute(\"\"\"\n",
    "                            UPDATE writers\n",
    "                            SET \n",
    "                                \"actual_subscribers\" = %s\n",
    "                            WHERE id = %s;\n",
    "                        \"\"\", (subscriber_count, user_id))\n",
    "                        conn.commit()\n",
    "                except psycopg2.Error as e:\n",
    "                    logging.error(f\"Database error: {e}\")\n",
    "                    conn.rollback()\n",
    "\n",
    "                break  # Exit the loop early if 'subscribers' is not found\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                if e.response is not None and e.response.status_code == 429:  # Rate limit error\n",
    "                    logging.warning(f\"Rate limit reached at {handle_url}. Retrying in {2 ** retries} seconds.\")\n",
    "                    time.sleep(min(2 ** retries, 10))  # Exponential backoff, max 60 seconds\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    logging.warning(f\"Error fetching {handle_url}: {e}. Skipping to next URL.\")\n",
    "                    break  # Skip to the next URL on any error other than rate limit\n",
    "            finally:\n",
    "                conn_pool.putconn(conn)\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT id, handle_url FROM writers WHERE actual_subscribers IS NULL;\")\n",
    "            user_ids = cur.fetchall()\n",
    "            for row in user_ids:\n",
    "                user_id = row[0]  # Extract the user ID from the tuple\n",
    "                handle_url = row[1]\n",
    "                get_subscriber_count(handle_url, user_id, conn)\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Database connection error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape following and followers lists (redundant if not doing network thing)\n",
    "import psycopg2\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "db_params = {\n",
    "    'host': \"localhost\",\n",
    "    'database': \"postgres\",\n",
    "    'user': \"cardigan\"\n",
    "}\n",
    "\n",
    "def random_sleep():\n",
    "    time.sleep(random.uniform(0.3, 0.9))\n",
    "\n",
    "def fetch_json(user_id, list_type):\n",
    "    url = f\"https://substack.com/api/v1/user/{user_id}/subscriber-lists?lists={list_type}\"\n",
    "    max_retries = 5\n",
    "    retry_delay = 0.5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                random_sleep()\n",
    "                return response.json()\n",
    "            else:\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}, retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay *= 2\n",
    "    return None\n",
    "\n",
    "def extract_author_ids(json_data):\n",
    "    author_ids = []\n",
    "    if 'subscriberLists' in json_data:\n",
    "        for subscriber_list in json_data['subscriberLists']:\n",
    "            if 'groups' in subscriber_list:\n",
    "                for group in subscriber_list['groups']:\n",
    "                    if 'users' in group:\n",
    "                        for user in group['users']:\n",
    "                            author_ids.append(user['id'])\n",
    "    return list(set(author_ids))\n",
    "\n",
    "def process_batch(batch, conn):\n",
    "    updated_count = 0\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            for user_id in batch:\n",
    "                utf_json = fetch_json(user_id, 'following')\n",
    "                uft_json = fetch_json(user_id, 'followers')\n",
    "\n",
    "                users_following_them_set = extract_author_ids(utf_json) if utf_json else []\n",
    "                users_they_follow_set = extract_author_ids(uft_json) if uft_json else []\n",
    "\n",
    "                cursor.execute(\"\"\"\n",
    "                    UPDATE writers\n",
    "                    SET users_they_follow_set = %s::int[], users_following_them_set = %s::int[]\n",
    "                    WHERE id = %s;\n",
    "                \"\"\", (users_they_follow_set, users_following_them_set, user_id))\n",
    "                if cursor.rowcount > 0:\n",
    "                    updated_count += 1\n",
    "            conn.commit()\n",
    "            print(f\"Total IDs updated in this batch: {updated_count}\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        conn.rollback()\n",
    "    return updated_count\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT id FROM writers WHERE actual_subscribers > 100;\")\n",
    "            user_ids = [row[0] for row in cur.fetchall()]\n",
    "            for i in range(0, len(user_ids), 30):\n",
    "                batch = user_ids[i:i+30]\n",
    "                print(f\"{len(user_ids)-i} ids left to scrape\")\n",
    "                rows_processed = process_batch(batch, conn)\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Database connection error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_boo_followers rows updated: 0\n",
      "Database connection error: there is no unique or exclusion constraint matching the ON CONFLICT specification\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do network thing (redundant)\n",
    "import psycopg2\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "db_params = {\n",
    "    'host': \"localhost\",\n",
    "    'database': \"postgres\",\n",
    "    'user': \"cardigan\"\n",
    "}\n",
    "\n",
    "def random_sleep():\n",
    "    time.sleep(random.uniform(0.3, 0.9))\n",
    "\n",
    "def fetch_json(user_id, list_type):\n",
    "    url = f\"https://substack.com/api/v1/user/{user_id}/subscriber-lists?lists={list_type}\"\n",
    "    max_retries = 5\n",
    "    retry_delay = 0.5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                random_sleep()\n",
    "                return response.json()\n",
    "            elif response.status_code == 429:  # Handle rate limit\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2\n",
    "            elif response.status_code >= 500:  # Retry on server errors\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2\n",
    "            else:  # For other errors, don't retry\n",
    "                break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}, retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay *= 2\n",
    "    return None\n",
    "\n",
    "def extract_author_ids(json_data):\n",
    "    author_ids = []\n",
    "    if json_data and 'subscriberLists' in json_data:\n",
    "        for subscriber_list in json_data['subscriberLists']:\n",
    "            if 'groups' in subscriber_list:\n",
    "                for group in subscriber_list['groups']:\n",
    "                    if 'users' in group:\n",
    "                        for user in group['users']:\n",
    "                            author_ids.append(user['id'])\n",
    "    return list(set(author_ids))\n",
    "\n",
    "def process_batch(batch, conn):\n",
    "    updated_count = 0\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            for user_id in batch:\n",
    "                utf_json = fetch_json(user_id, 'following')\n",
    "                uft_json = fetch_json(user_id, 'followers')\n",
    "\n",
    "                users_following_them_set = extract_author_ids(utf_json) if utf_json else []\n",
    "                users_they_follow_set = extract_author_ids(uft_json) if uft_json else []\n",
    "\n",
    "                cursor.execute(\"\"\"\n",
    "                    UPDATE followed_by_writers\n",
    "                    SET users_they_follow_set = %s::int[], users_following_them_set = %s::int[]\n",
    "                    WHERE id = %s;\n",
    "                \"\"\", (users_they_follow_set, users_following_them_set, user_id))\n",
    "                if cursor.rowcount > 0:\n",
    "                    updated_count += 1\n",
    "            conn.commit()\n",
    "            print(f\"Total IDs updated in this batch: {updated_count}\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        conn.rollback()\n",
    "    return updated_count\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            try:\n",
    "                cur.execute(\"SELECT COUNT(*) FROM writers;\")\n",
    "                values_in_boo = cur.fetchone()[0]\n",
    "            except psycopg2.Error as e:\n",
    "                print(f\"Error executing query: {e}\")\n",
    "                conn.rollback()\n",
    "\n",
    "            while values_in_boo <= 15000:\n",
    "                cur.execute(\"\"\"\n",
    "                INSERT INTO followed_by_writers (id)\n",
    "                SELECT UNNEST(users_they_follow_set::int[])\n",
    "                FROM writers\n",
    "                ON CONFLICT (id) DO NOTHING;\n",
    "                \"\"\")\n",
    "\n",
    "                cur.execute(\"\"\"\n",
    "                    UPDATE followed_by_writers fb\n",
    "                    SET\n",
    "                        users_following_them_set = subq.users_following_them,\n",
    "                        users_they_follow_set = subq.users_they_follow\n",
    "                    FROM \n",
    "                        (SELECT \n",
    "                            coalesce(ca.\"User_ID\", c_a.user_id, wn1.id, wn2.id, wn3.id) as id,\n",
    "                            COALESCE(ca.users_following_them, c_a.users_following_them, wn1.users_following_them, wn2.users_following_them, wn3.users_following_them) as users_following_them,\n",
    "                            COALESCE(ca.users_they_follow, c_a.users_they_follow, wn1.users_they_follow, wn2.users_they_follow, wn3.users_they_follow) as users_they_follow\n",
    "                        FROM \n",
    "                            following_lists ca\n",
    "                            FULL OUTER JOIN core_augmented c_a ON ca.\"User_ID\" = c_a.user_id\n",
    "                            FULL OUTER JOIN well_networked_1 wn1 ON ca.\"User_ID\" = wn1.id\n",
    "                            FULL OUTER JOIN well_networked_2 wn2 ON ca.\"User_ID\" = wn2.id\n",
    "                            FULL OUTER JOIN well_networked_3 wn3 ON ca.\"User_ID\" = wn3.id\n",
    "                        ) AS subq\n",
    "                    WHERE                         fb.id = subq.id\n",
    "                        AND (fb.users_following_them_set IS DISTINCT FROM subq.users_following_them\n",
    "                            OR fb.users_they_follow_set IS DISTINCT FROM subq.users_they_follow)\n",
    "                \"\"\")\n",
    "\n",
    "                cur.execute(\"SELECT id FROM followed_by_writers WHERE users_they_follow_set IS NULL OR users_following_them_set IS NULL\")\n",
    "                remaining_ids = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "                if remaining_ids:\n",
    "                    rows_committed_fbboo = 0\n",
    "                    for i in range(0, len(remaining_ids), 30):\n",
    "                        batch = remaining_ids[i:i+30]\n",
    "                        print(f\"{len(remaining_ids)-i} ids left to scrape\")\n",
    "                        rows_processed = process_batch(batch, conn)\n",
    "                        rows_committed_fbboo += rows_processed\n",
    "\n",
    "                    print(f\"New following scraped for followed_by_writers: {rows_committed_fbboo}\")\n",
    "                    \n",
    "                    cur.execute(\"\"\"\n",
    "                        UPDATE followed_by_writers fb\n",
    "                        SET num_boo_followers = subq.new_value\n",
    "                        FROM (\n",
    "                            SELECT fbb.id, COUNT(*) as new_value\n",
    "                            FROM followed_by_writers fbb\n",
    "                            JOIN writers boo ON boo.id = ANY(fbb.users_following_them_set::int[])\n",
    "                            GROUP BY fbb.id\n",
    "                        ) subq\n",
    "                        WHERE fb.id = subq.id\n",
    "                        AND COALESCE(fb.num_boo_followers, 0) != subq.new_value;\n",
    "                    \"\"\")\n",
    "\n",
    "                rows_updated = cur.rowcount\n",
    "                print(f\"num_boo_followers rows updated: {rows_updated}\")\n",
    "\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO writers (id, users_they_follow_set, users_following_them_set)\n",
    "                    SELECT id, users_they_follow_set, users_following_them_set \n",
    "                    FROM (\n",
    "                        SELECT id, users_they_follow_set, users_following_them_set \n",
    "                        FROM followed_by_writers\n",
    "                        WHERE id NOT IN (SELECT id FROM writers)\n",
    "                        ORDER BY num_boo_followers DESC\n",
    "                        LIMIT 1\n",
    "                    ) AS subquery\n",
    "                    ON CONFLICT (id) DO NOTHING;\n",
    "                \"\"\")\n",
    "                print(f\"1 row committed to writers\")\n",
    "\n",
    "                conn.commit()\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Database connection error: {e}\")\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapes subdomain url using id (redundant)\n",
    "import requests\n",
    "import psycopg2\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'host': \"localhost\",\n",
    "    'database': \"postgres\",\n",
    "    'user': \"cardigan\"\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "def read_JSON(data, conn, user_id):\n",
    "    published_bylines = data['posts'][0].get('publishedBylines', [])\n",
    "    if published_bylines and 'publicationUsers' in published_bylines[0]:\n",
    "        subdomain = published_bylines[0]['publicationUsers'][0]['publication']['subdomain']\n",
    "    else:\n",
    "        subdomain = ''\n",
    "\n",
    "    return (f\"https://{subdomain}.substack.com\" if subdomain else '')\n",
    "\n",
    "\n",
    "def scrape_and_update(user_id, conn):\n",
    "    subdomain_url = ''\n",
    "    # API Call for handle, subdomain\n",
    "    api_url = f\"https://substack.com/api/v1/profile/posts?profile_user_id={user_id}&offset=0\"\n",
    "    \n",
    "    # https://substack.com/api/v1/profile/posts?profile_user_id=2923623&offset=0\n",
    "\n",
    "    # logging.info(api_url)\n",
    "    attempt = 0\n",
    "    while attempt < 2:\n",
    "        try:\n",
    "            response = requests.get(api_url)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data['posts']:\n",
    "                    subdomain_url = read_JSON(data, conn, user_id)\n",
    "                    # logging.info(f\"subdomain: {subdomain_url}\")\n",
    "                    break\n",
    "            else:\n",
    "                time.sleep(min(2 ** attempt, 10))\n",
    "                attempt += 1\n",
    "                logging.info(f\"No response, trying again with attempt {attempt}\")\n",
    "        except requests.RequestException as e:\n",
    "            logging.warning(f\"handle/subdomain attempt {attempt + 1}: Error with URL {api_url}. Exception: {e}\")\n",
    "            time.sleep(min(1.4 ** attempt, 6))\n",
    "            attempt += 1\n",
    "\n",
    "    # Update the database\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                UPDATE writers\n",
    "                SET \n",
    "                    \"subdomain_url\" = %s\n",
    "                WHERE id = %s;\n",
    "            \"\"\", (subdomain_url, user_id))\n",
    "            conn.commit()\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"Database error: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Main scraping and updating loop\n",
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT id FROM writers WHERE subdomain_url IS NULL AND subscribers > 10;\")\n",
    "            user_ids = cur.fetchall()\n",
    "            for row in user_ids:\n",
    "                user_id = row[0]  # Extract the user ID from the tuple\n",
    "                scrape_and_update(user_id, conn)\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"Database connection error: {e}\")\n",
    "    \n",
    "logging.info(\"Completed processing all rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape rss feed, post likes, add to top_writer_posts\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import html\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT subdomain_url FROM writers WHERE subdomain_url IS NOT NULL AND actual_subscribers > 10;\")\n",
    "subdomain_urls = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "# Function to fetch reactions\n",
    "def fetch_reactions(url):\n",
    "    response = requests.get(url + \"/archive\")\n",
    "    reactions_dict = {}\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        scripts = soup.find_all('script')\n",
    "    for script in scripts:\n",
    "        if 'window._preloads' in script.text:\n",
    "            json_str = script.text.split('window._preloads        = JSON.parse(', 1)[1]\n",
    "            json_str = json_str.rstrip(');')\n",
    "            # Unescape the JSON string before loading it\n",
    "            json_str = json.loads(json_str)\n",
    "            data = json.loads(json_str)\n",
    "            print(data)\n",
    "            posts = data.get('page', {}).get('postsModule', {}).get('posts', [])\n",
    "            if posts:\n",
    "                for post in posts:\n",
    "                    title = post.get('title')\n",
    "                    reactions = post.get('reactions', {}).get('\\u2764', 0)\n",
    "                    reactions_dict[title] = reactions\n",
    "    return reactions_dict\n",
    "\n",
    "# Loop through the URLs to fetch the XML content\n",
    "for url in subdomain_urls:\n",
    "    try:\n",
    "        reactions_dict = fetch_reactions(url)\n",
    "        response = requests.get(url + \"/feed\")\n",
    "        if response.status_code == 200 and 'application/xml' in response.headers.get('Content-Type', ''):\n",
    "            root = ET.fromstring(response.content)\n",
    "            \n",
    "            channel_title = root.find('./channel/title').text if root.find('./channel/title') is not None else None\n",
    "            channel_description = root.find('./channel/description').text if root.find('./channel/description') is not None else None\n",
    "            channel_author = root.find('./channel/copyright').text if root.find('./channel/copyright') is not None else None\n",
    "\n",
    "            # Loop through each item (post) in the XML\n",
    "            for item in root.findall('./channel/item'):\n",
    "                item_title = item.find('title').text if item.find('title') is not None else None\n",
    "                item_description = item.find('description').text if item.find('description') is not None else None\n",
    "                item_pubdate = item.find('pubDate').text if item.find('pubDate') is not None else None\n",
    "                if item.find('content:encoded', namespaces={'content': 'http://purl.org/rss/1.0/modules/content/'}):\n",
    "                    soup = BeautifulSoup(item.find('content:encoded', namespaces={'content': 'http://purl.org/rss/1.0/modules/content/'}).text, 'html.parser')\n",
    "                    item_content = soup.get_text()\n",
    "                reactions = reactions_dict.get(item_title, 0)\n",
    "                item_content = html.unescape(item_content) if item_content else None\n",
    "\n",
    "                # Insert the extracted data into the PostgreSQL table\n",
    "                query = sql.SQL(\"INSERT INTO top_writer_posts (channel_title, channel_description, channel_author, item_title, item_description, item_pubdate, item_content, reactions) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\")\n",
    "                cur.execute(query, (channel_title, channel_description, channel_author, item_title, item_description, item_pubdate, item_content, reactions))\n",
    "                conn.commit()\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch XML from {url} or not an XML response.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Close the database connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch XML from https://discoursemagazine.substack.com or not an XML response.\n"
     ]
    }
   ],
   "source": [
    "#Scrape rss feed, reactions, add to top_writer_posts\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import html\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='scraping.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT subdomain_url FROM writers WHERE subdomain_url IS NOT NULL AND scraped IS NULL AND subscribers > 10;\")\n",
    "subdomain_urls = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "# Function to fetch reactions\n",
    "def fetch_reactions(url):\n",
    "    titles_and_reactions = []\n",
    "    max_retries = 5\n",
    "    retry_delay = 0.5  # Initial delay is 0.5 seconds\n",
    "    reactions_dict = {}\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url + \"/archive\")\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                scripts = soup.find_all('script')\n",
    "                for script in scripts:\n",
    "                    if 'window._preloads        = JSON.parse(' in script.text:\n",
    "                        json_str = script.text.split('window._preloads        = JSON.parse(', 1)[1]\n",
    "                        json_str = json_str.rstrip(');')\n",
    "                        # Unescape the JSON string before loading it\n",
    "                        json_str = json.loads(json_str)\n",
    "                        data = json.loads(json_str)\n",
    "                        titles_and_reactions = [(post[\"title\"], post[\"reaction_count\"]) for post in data.get(\"newPosts\", [])]\n",
    "                        return dict(titles_and_reactions)\n",
    "            elif response.status_code == 429:  # Too Many Requests\n",
    "                print(f\"Too many requests, retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Double the delay each time\n",
    "            else:\n",
    "                return reactions_dict  # Return empty dict for other errors\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}, retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay *= 2\n",
    "    return reactions_dict\n",
    "    \n",
    "\n",
    "# Loop through the URLs to fetch the XML content\n",
    "for url in subdomain_urls:\n",
    "    max_retries = 5\n",
    "    retry_delay = 0.5  # Initial delay is 0.5 seconds\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            reactions_dict = fetch_reactions(url)\n",
    "            response = requests.get(url + \"/feed\")\n",
    "            if response.status_code == 200 and 'application/xml' in response.headers.get('Content-Type', ''):\n",
    "                root = ET.fromstring(response.content)\n",
    "                \n",
    "                channel_title = root.find('./channel/title').text if root.find('./channel/title') is not None else None\n",
    "                # check if channel_title matches a value in channel_title column of top_writer_posts\n",
    "                cur.execute(\"SELECT 1 FROM top_writer_posts WHERE channel_title = %s LIMIT 1;\", (channel_title,))\n",
    "                if cur.fetchone() is not None:\n",
    "                    # If channel_title exists, update 'scraped' to TRUE and skip the rest of the loop\n",
    "                    cur.execute(\"UPDATE writers SET scraped = TRUE WHERE subdomain_url = %s;\", (url,))\n",
    "                    conn.commit()\n",
    "                    logging.info(f\"Set scraped to TRUE for subdomain_url: {url}\")\n",
    "                    continue\n",
    "\n",
    "                channel_description = root.find('./channel/description').text if root.find('./channel/description') is not None else None\n",
    "                channel_author = root.find('./channel/copyright').text if root.find('./channel/copyright') is not None else None\n",
    "\n",
    "                # Loop through each item (post) in the XML\n",
    "                for item in root.findall('./channel/item'):\n",
    "                    item_title = item.find('title').text if item.find('title') is not None else None\n",
    "                    item_description = item.find('description').text if item.find('description') is not None else None\n",
    "                    item_pubdate = item.find('pubDate').text if item.find('pubDate') is not None else None\n",
    "                    item_content = item.find('content:encoded', namespaces={'content': 'http://purl.org/rss/1.0/modules/content/'}).text if item.find('content:encoded', namespaces={'content': 'http://purl.org/rss/1.0/modules/content/'}) is not None else None\n",
    "                    # Remove HTML tags and NCRs from item_content\n",
    "                    if item_content:\n",
    "                        soup = BeautifulSoup(item_content, 'html.parser')\n",
    "                        item_content = soup.get_text()\n",
    "                        item_content = html.unescape(item_content)\n",
    "                    if reactions:\n",
    "                        reactions = reactions_dict.get(item_title, 0)\n",
    "                    else: \n",
    "                        reactions = 1\n",
    "                    \n",
    "                    cur.execute(\"UPDATE writers SET scraped = TRUE WHERE subdomain_url = %s;\", (url,))\n",
    "                        # Insert the extracted data into the PostgreSQL table\n",
    "                    query = sql.SQL(\"INSERT INTO top_writer_posts (channel_title, channel_description, channel_author, item_title, item_description, item_pubdate, item_content, reactions) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\")\n",
    "                    cur.execute(query, (channel_title, channel_description, channel_author, item_title, item_description, item_pubdate, item_content, reactions))\n",
    "                    conn.commit()\n",
    "                    logging.info(f\"Added posts for {url}\")\n",
    "                break\n",
    "            elif response.status_code == 429:  # Too Many Requests\n",
    "                print(f\"Too many requests, retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Double the delay each time\n",
    "            else:\n",
    "                print(f\"Failed to fetch XML from {url} or not an XML response.\")\n",
    "                break  # Break the loop for other errors\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}, retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay *= 2\n",
    "\n",
    "# Close the database connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape rss feed, reactions, add to top_writer_posts\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import html\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='scraping.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT subdomain_url FROM writers WHERE subdomain_url IS NOT NULL AND scraped IS NULL AND subscribers > 10;\")\n",
    "subdomain_urls = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "# Function to fetch reactions\n",
    "def fetch_reactions(url):\n",
    "    titles_and_reactions = []\n",
    "    max_retries = 5\n",
    "    retry_delay = 0.5  # Initial delay is 0.5 seconds\n",
    "    reactions_dict = {}\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url + \"/archive\")\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                scripts = soup.find_all('script')\n",
    "                for script in scripts:\n",
    "                    if 'window._preloads        = JSON.parse(' in script.text:\n",
    "                        json_str = script.text.split('window._preloads        = JSON.parse(', 1)[1]\n",
    "                        json_str = json_str.rstrip(');')\n",
    "                        # Unescape the JSON string before loading it\n",
    "                        json_str = json.loads(json_str)\n",
    "                        data = json.loads(json_str)\n",
    "                        titles_and_reactions = [(post[\"title\"], post[\"reaction_count\"]) for post in data.get(\"newPosts\", [])]\n",
    "                        return dict(titles_and_reactions)\n",
    "            elif response.status_code == 429:  # Too Many Requests\n",
    "                print(f\"Too many requests, retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Double the delay each time\n",
    "            else:\n",
    "                return reactions_dict  # Return empty dict for other errors\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}, retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay *= 2\n",
    "    return reactions_dict\n",
    "    \n",
    "\n",
    "# Loop through the URLs to fetch the XML content\n",
    "for url in subdomain_urls:\n",
    "    max_retries = 5\n",
    "    retry_delay = 0.5  # Initial delay is 0.5 seconds\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            reactions_dict = fetch_reactions(url)\n",
    "            response = requests.get(url + \"/feed\")\n",
    "            if response.status_code == 200 and 'application/xml' in response.headers.get('Content-Type', ''):\n",
    "                root = ET.fromstring(response.content)\n",
    "                \n",
    "                channel_title = root.find('./channel/title').text if root.find('./channel/title') is not None else None\n",
    "                # check if channel_title matches a value in channel_title column of top_writer_posts\n",
    "                cur.execute(\"SELECT 1 FROM top_writer_posts WHERE channel_title = %s LIMIT 1;\", (channel_title,))\n",
    "                if cur.fetchone() is not None:\n",
    "                    # If channel_title exists, update 'scraped' to TRUE and skip the rest of the loop\n",
    "                    cur.execute(\"UPDATE writers SET scraped = TRUE WHERE subdomain_url = %s;\", (url,))\n",
    "                    conn.commit()\n",
    "                    logging.info(f\"Set scraped to TRUE for subdomain_url: {url}\")\n",
    "                    continue\n",
    "\n",
    "                channel_description = root.find('./channel/description').text if root.find('./channel/description') is not None else None\n",
    "                channel_author = root.find('./channel/copyright').text if root.find('./channel/copyright') is not None else None\n",
    "\n",
    "                # Loop through each item (post) in the XML\n",
    "                for item in root.findall('./channel/item'):\n",
    "                    item_title = item.find('title').text if item.find('title') is not None else None\n",
    "                    item_description = item.find('description').text if item.find('description') is not None else None\n",
    "                    item_pubdate = item.find('pubDate').text if item.find('pubDate') is not None else None\n",
    "                    item_content = item.find('content:encoded', namespaces={'content': 'http://purl.org/rss/1.0/modules/content/'}).text if item.find('content:encoded', namespaces={'content': 'http://purl.org/rss/1.0/modules/content/'}) is not None else None\n",
    "                    # Remove HTML tags and NCRs from item_content\n",
    "                    if item_content:\n",
    "                        soup = BeautifulSoup(item_content, 'html.parser')\n",
    "                        item_content = soup.get_text()\n",
    "                        item_content = html.unescape(item_content)\n",
    "                    if reactions:\n",
    "                        reactions = reactions_dict.get(item_title, 0)\n",
    "                    else: \n",
    "                        reactions = 1\n",
    "                    \n",
    "                    cur.execute(\"UPDATE writers SET scraped = TRUE WHERE subdomain_url = %s;\", (url,))\n",
    "                        # Insert the extracted data into the PostgreSQL table\n",
    "                    query = sql.SQL(\"INSERT INTO top_writer_posts (channel_title, channel_description, channel_author, item_title, item_description, item_pubdate, item_content, reactions) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\")\n",
    "                    cur.execute(query, (channel_title, channel_description, channel_author, item_title, item_description, item_pubdate, item_content, reactions))\n",
    "                    conn.commit()\n",
    "                    logging.info(f\"Added posts for {url}\")\n",
    "                break\n",
    "            elif response.status_code == 429:  # Too Many Requests\n",
    "                print(f\"Too many requests, retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Double the delay each time\n",
    "            else:\n",
    "                print(f\"Failed to fetch XML from {url} or not an XML response.\")\n",
    "                break  # Break the loop for other errors\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}, retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay *= 2\n",
    "\n",
    "# Close the database connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many requests, retrying in 0.5 seconds...\n",
      "Too many requests, retrying in 1.0 seconds...\n",
      "Too many requests, retrying in 2.0 seconds...\n",
      "Too many requests, retrying in 0.5 seconds...\n",
      "Too many requests, retrying in 1.0 seconds...\n",
      "Too many requests, retrying in 2.0 seconds...\n",
      "Too many requests, retrying in 4.0 seconds...\n",
      "Too many requests, retrying in 8.0 seconds...\n",
      "Too many requests, retrying in 0.5 seconds...\n",
      "Too many requests, retrying in 1.0 seconds...\n",
      "Too many requests, retrying in 2.0 seconds...\n",
      "Too many requests, retrying in 4.0 seconds...\n",
      "Too many requests, retrying in 8.0 seconds...\n",
      "Too many requests, retrying in 0.5 seconds...\n",
      "Too many requests, retrying in 0.5 seconds...\n",
      "Too many requests, retrying in 1.0 seconds...\n",
      "Too many requests, retrying in 2.0 seconds...\n",
      "Too many requests, retrying in 4.0 seconds...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# scrape act posts\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='scraping.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Load the JSON data\n",
    "with open('act_posts.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Loop through the JSON objects\n",
    "for post in data:\n",
    "    # Extract the data from the JSON object\n",
    "    post_date = post['post_date']\n",
    "    reactions = post['reactions']['❤'] if '❤' in post['reactions'] else 0\n",
    "    publication_name = post['publishedBylines'][0]['publicationUsers'][0]['publication']['name']\n",
    "    author_name = post['publishedBylines'][0]['name']\n",
    "    url = post['canonical_url']\n",
    "\n",
    "    # Fetch the post content\n",
    "    max_retries = 5\n",
    "    retry_delay = 0.5  # Initial delay is 0.5 seconds\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                content_div = soup.find('div', {'class': 'available-content'})\n",
    "                item_content = content_div.get_text() if content_div else ''\n",
    "                item_content = html.unescape(item_content)  # Remove NCRs\n",
    "                break\n",
    "            elif response.status_code == 429:  # Too Many Requests\n",
    "                print(f\"Too many requests, retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Double the delay each time\n",
    "            else:\n",
    "                print(f\"Failed to fetch content from {url}.\")\n",
    "                item_content = None\n",
    "                break  # Break the loop for other errors\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}, retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay *= 2\n",
    "\n",
    "    # Insert the extracted data into the PostgreSQL table\n",
    "    # Check if the post already exists in the database\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT 1 FROM top_writer_posts \n",
    "        WHERE item_pubdate = %s AND item_title = %s\n",
    "        LIMIT 1;\n",
    "    \"\"\", (post_date, post['title']))\n",
    "\n",
    "    # If the post does not exist, insert it\n",
    "    if cur.fetchone() is None:\n",
    "        query = sql.SQL(\"\"\"\n",
    "            INSERT INTO top_writer_posts \n",
    "            (channel_title, channel_description, channel_author, item_title, item_description, item_pubdate, item_content, reactions) \n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\")\n",
    "        cur.execute(query, (publication_name, None, author_name, post['title'], post['description'], post_date, item_content, reactions))\n",
    "        conn.commit()\n",
    "        logging.info(f\"Added post: {post['title']}\")\n",
    "    else:\n",
    "        logging.info(f\"Skipped duplicate post: {post['title']}\")\n",
    "\n",
    "    # query = sql.SQL(\"INSERT INTO top_writer_posts (channel_title, channel_description, channel_author, item_title, item_description, item_pubdate, item_content, reactions) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\")\n",
    "    # cur.execute(query, (publication_name, None, author_name, post['title'], post['description'], post_date, item_content, reactions))\n",
    "    # conn.commit()\n",
    "    # logging.info(f\"Added post: {post['title']}\")\n",
    "\n",
    "# Close the database connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SQL query\n",
    "cur.execute(\"SELECT item_content FROM top_writer_posts ORDER BY LENGTH(item_content);\")\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Extract the lengths of the item_content\n",
    "lengths = [len(result[0]) for result in results]\n",
    "\n",
    "bins = range(0, 25000 + 500, 500)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lengths, bins=bins, edgecolor='black')\n",
    "plt.title('Length of item_content in top_writer_posts')\n",
    "plt.xlabel('Length of item_content')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show reactions to length ratio\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SQL query\n",
    "cur.execute(\"SELECT LENGTH(item_content), reactions FROM top_writer_posts;\")\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Extract the lengths of the item_content and the number of reactions\n",
    "lengths = [result[0] for result in results]\n",
    "reactions = [result[1] for result in results]\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lengths, reactions)\n",
    "plt.title('Number of Reactions vs Length of item_content in top_writer_posts')\n",
    "plt.xlabel('Length of item_content')\n",
    "plt.ylabel('Number of Reactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a blog post titled \"A Week in the War on Woman: Monday 27th November - Sunday 3rd December GOOD NEWS SUPPLEMENT\" with description \"Another bumper edition packed to the gills with good news stories from the gender beat this week. Enjoy! Players With Balls We have reported previously on the courageous heroines of the professional pool circuit. Alexandra Cunha, one of the world’s top-ranking female pool stars, is\" in a style that includes:\n",
      "- Briefly describe the structure & pacing: Adhere to a dynamic and quick-paced structure, start with an attention-grabbing introduction about the recent achievements and events related to women's rights and gender-related issues. Craft punchy, brief paragraphs that maintain momentum and weave through the good news pieces with fluid transitions.\n",
      "\n",
      "- Detail the tone and its relation to content: Employ an assertive and celebratory tone consistent with the good news theme. Use positive language to emphasize advancements and victories in gender equality—cheering on the achievements while maintaining an informative stance.\n",
      "\n",
      "- Outline the unique style elements: Integrate direct quotes from influential figures to underscore points, offer succinct commentary to maintain reader's interest swiftly, and use rhetorical questions occasionally to engage with the readers. Be sure to include colloquial phrases or slang that relates to the topic, such as \"Players With Balls,\" to create a vivacious and bold style.\n",
      "\n",
      "- Explain the use and impact of identified literary device: Employ a strategic use of hyperbole to accentuate the impact of the good news stories, thus elevating the feeling of triumph and progression. Allusions to well-known activists and events can also create a rich context and resonate deeply with the informed reader, strengthening the blog post's underlying message of empowerment.\n",
      "{'tags': ['Gender Equality', 'Positive News', 'Sports', 'Feminism']}\n",
      "Write a blog post titled \"I'm like a library book\" with description \"so check me out\" in a style that includes:\n",
      "- A structure & pacing that starts with a personal anecdote, transitions into historical or factual content, weaves in cultural references, and concludes with a reflective or action-inspiring epilogue.\n",
      "- A tone that blends humor with earnestness, juxtaposing light-hearted personal narratives with a genuine appreciation for the subject matter.\n",
      "- Unique style elements such as conversational language, vivid imagery, and playful asides that engage the reader in a friendly and informative manner.\n",
      "- The use of literary devices like metaphors and personification to deepen the emotional connection and enhance the narrative, and anecdotes to illustrate points and add a human element to the discussion.\n",
      "{'tags': ['Travel Writing', 'Humor', 'Adventure', 'Reflective', 'Irish Honeymoon']}\n",
      "Write a blog post titled \"Is GPT-4 getting worse over time?\" with description \"A new paper going viral has been widely misinterpreted\" in a style that includes:\n",
      "- Briefly describe the structure & pacing: Start with an intro that presents the controversial claim and its significance, followed by a deeper examination of the paper's methodology and findings in the body. Finish with a conclusion that recaps the main points and offers additional commentary on the paper's impact.\n",
      "- Detail the tone and its relation to content: Craft the tone to be analytical and slightly critical, with an undercurrent of educational intent. Ensure the tone conveys an awareness of nuance and complexity surrounding AI performance evaluations.\n",
      "- Outline the unique style elements: Utilize accessible language with occasional technical details to bridge expert and layperson understanding. Include hypothetical examples and direct addresses to the reader to foster engagement.\n",
      "- Explain the use and impact of identified literary devices: Employ analogies to clarify AI concepts, use allusion to prior incidents in AI development for context, adopt a measured dose of hyperbole to underscore misconceptions, and introduce a potential foreshadowing of the implications of unreliable AI performance metrics.\n",
      "{'tags': ['AI Misinterpretation', 'Chatbot Capabilities vs. Behavior', 'Paper Analysis', 'GPT-4 Performance']}\n",
      "Write a blog post titled \"Completely letting go of control\" with description \"The last year in my brain, revisited\" in a style that includes:\n",
      "- A nonlinear narrative structure reflecting the ebb and flow of the meditation journey, with varying pacing that mirrors the author's fluctuating experiences of control and surrender.\n",
      "- A contemplative and introspective tone with moments of levity, drawing the reader into the deeply personal and transformative nature of the events.\n",
      "- A distinct style marked by vulnerable self-disclosure, philosophical musings interspersed with everyday anecdotes, and a conversational yet poetic voice that invites readers into the author's internal world.\n",
      "- An effective use of metaphor and introspective questioning, contributing to the depth and relatability of the writer's insights on the nature of existence and self.\n",
      "{'tags': ['Meditation', 'Self-reflection', 'Contemplative practice', 'Personal growth', 'Surrendering control']}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create prompt and tags, append to JSONL\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# openai.api_key = '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' #My Key\n",
    "openai.api_key = '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' #Alt Key\n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query to fetch the posts\n",
    "query = \"SELECT item_content, item_title, item_description FROM top_writer_posts WHERE gpt3 IS NULL AND gpt4 IS NULL AND reactions > 100\"\n",
    "cur.execute(query)\n",
    "posts = cur.fetchall()\n",
    "\n",
    "def generate_prompt(post):\n",
    "    # Generate user prompt\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "    Title: {post[1]}\n",
    "    Description: {post[2]}\n",
    "    Blog post: {post[0]}\n",
    "\n",
    "    Analyze the given blog post to identify unique aspects of the writer's approach and writing style, focusing on:\n",
    "\n",
    "    0. Content Analysis: Identify major topics, events, and other content. Describe how these elements interrelate and contribute to the overall narrative or message.\n",
    "    1. Structure and Pacing: Examine sections such as the opening, body, and ending. Assess whether the pacing is fast, slow, or varied, and note how transitions between sections are handled for seamless flow.\n",
    "    2. Tone Analysis: Determine the tone or tones (e.g., serious, humorous) used and explain how they affect reader engagement and perception, particularly in relation to the topics covered.\n",
    "    3. Unique Style Identification: Identify specific elements that make the writer's style unique, such as language choices, sentence structure, and thematic focus. Consider how these elements serve the content or the author's intent.\n",
    "    4. Literary Devices: Identify literary devices like analogy, allegory, hyperbole, allusion, and foreshadowing. Discuss how each device contributes to the overall impact or message of the post.\n",
    "\n",
    "    Based on your analysis, create a prompt for a new blog post in the same style. The prompt should follow this template:\n",
    "\n",
    "    'Write a blog post titled \"{post[1]}\" with description \"{post[2]}\" in a style that includes:\n",
    "    - Briefly describe the structure & pacing\n",
    "    - Detail the tone and its relation to content\n",
    "    - Outline the unique style elements\n",
    "    - Explain the use and impact of identified literary device]'\n",
    "\n",
    "    Return only the new prompt and nothing else.\n",
    "                \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": post[0]}\n",
    "            ]\n",
    "        )\n",
    "        user_content = response.choices[0].message.content\n",
    "        print(user_content)\n",
    "        query = sql.SQL(\"\"\"\n",
    "        UPDATE top_writer_posts \n",
    "        SET gpt4 = %s\n",
    "        WHERE item_title = %s;\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in GPT-4 API call:\", e)\n",
    "        user_content = f\"write a blog post called {post[1]} with description {post[2]} in your personal style\\n\\n\"\n",
    "        query = sql.SQL(\"\"\"\n",
    "        UPDATE top_writer_posts \n",
    "        SET basic = %s\n",
    "        WHERE item_title = %s;\n",
    "        \"\"\")\n",
    "    return user_content, query\n",
    "\n",
    "def generate_tags(text, title_analyzed):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \n",
    "                 \"\"\"\n",
    "                Provide a JSON object with 1-5 short descriptive tags for the following text. Example:\\n\\n\n",
    "                \n",
    "                {\n",
    "                    \"tags\": [\n",
    "                        \"Creative Writing\",\n",
    "                        \"Medieval Steampunk with spiritual undertones\",\n",
    "                        \"Descriptive character-driven style\"\n",
    "                    ]\n",
    "                }\n",
    "                 \"\"\"\n",
    "                 },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "        print(response_json)\n",
    "    except Exception as e:\n",
    "        print(\"Error in calling GPT-3 API:\", e)\n",
    "        response_json = {\"tags\": None}\n",
    "    update_query = \"\"\"\n",
    "    UPDATE top_writer_posts \n",
    "    SET gpt_tags = array_cat(gpt_tags, %s)\n",
    "    WHERE item_title = %s\n",
    "    \"\"\"\n",
    "    data = (response_json['tags'] if response_json else [], title_analyzed)\n",
    "    cur.execute(update_query, data)\n",
    "    conn.commit()\n",
    "    return response_json\n",
    "\n",
    "# Check if the file exists\n",
    "file_exists = os.path.isfile('ava_training.jsonl')\n",
    "\n",
    "# Open the file in append mode if it exists, otherwise create a new file\n",
    "with open('ava_training.jsonl', 'a' if file_exists else 'w') as f:\n",
    "    for post in posts:\n",
    "        user_content, query = generate_prompt(post)\n",
    "        text_to_analyze = f\"Title:{post[1]}\\n\\nDescription:{post[2]}\\n\\nContent:{post[0][:500]}\"\n",
    "        response_json = generate_tags(text_to_analyze, post[1])\n",
    "        # System prompt\n",
    "        system_prompt = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "    You are an AI trained on a diverse set of blog posts. \n",
    "    Your task is to generate content in a style that matches the input prompt. \n",
    "    Be insightful and adapt your tone to the topic presented, ranging from formal to casual as needed.\n",
    "            \"\"\"\n",
    "        }\n",
    "        # Assistant content to append to JSONL\n",
    "        interaction = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": post[0]}\n",
    "        ]\n",
    "        # Write to the JSONL file\n",
    "        f.write(json.dumps({\"messages\": interaction}) + '\\n')\n",
    "\n",
    "        cur.execute(query, (user_content, post[1]))\n",
    "        conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace json fragments in list\n",
    "import psycopg2\n",
    "\n",
    "# Database connection\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Fetch data from the table\n",
    "    cursor.execute(\"SELECT item_pubdate, gpt_tags FROM top_writer_posts\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    for row in rows:\n",
    "        item_pubdate, gpt_tags = row\n",
    "        if gpt_tags is not None:\n",
    "            # Convert array to string and process it\n",
    "            tags_str = str(gpt_tags)\n",
    "            tags_str = tags_str.replace('{\"[', '').replace(']\"}', '')\n",
    "\n",
    "            # Splitting the string into an array\n",
    "            tags_list = [tag.strip() for tag in tags_str.split(',')]\n",
    "\n",
    "            # Update the row in the table\n",
    "            update_query = \"UPDATE top_writer_posts SET gpt_tags = %s WHERE item_pubdate = %s\"\n",
    "            cursor.execute(update_query, (tags_list, item_pubdate))\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace more json fragments in list\n",
    "import psycopg2\n",
    "\n",
    "# Database connection\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Fetch data from the table\n",
    "    cursor.execute(\"SELECT item_pubdate, gpt_tags FROM top_writer_posts\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    for row in rows:\n",
    "        item_pubdate, gpt_tags = row\n",
    "        if gpt_tags is not None:\n",
    "            # Process each tag\n",
    "            processed_tags = []\n",
    "            for tag in gpt_tags:\n",
    "                # Replace the unwanted substrings\n",
    "                tag = tag.replace(\"['\", \"'\").replace(\"']\", \"'\")\n",
    "                processed_tags.append(tag)\n",
    "\n",
    "            # Update the row in the table\n",
    "            update_query = \"UPDATE top_writer_posts SET gpt_tags = %s WHERE item_pubdate = %s\"\n",
    "            cursor.execute(update_query, (processed_tags, item_pubdate))\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated rows successfully.\n"
     ]
    }
   ],
   "source": [
    "# Replace None with empty\n",
    "import psycopg2\n",
    "\n",
    "# Database connection\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Update rows where gpt_tags contain 'None'\n",
    "    update_query = \"UPDATE top_writer_posts SET gpt_tags = ARRAY[]::text[] WHERE gpt_tags::text LIKE '%None%'\"\n",
    "    cursor.execute(update_query)\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "    print(\"Updated rows successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Looping in Education\"\n",
      "\"Food Industry Challenges\"\n",
      "\"3D Reconstruction\"\n",
      "'Culinary Inspiration'\n",
      "\"CEO Firing Reaction\"\n",
      "\"Reflections\"\n",
      "\"Societal Commentary\"\n",
      "\"Hope\"\n",
      "'Hope'\n",
      "\"Childhood Comfort Objects\"\n"
     ]
    }
   ],
   "source": [
    "# Print array items\n",
    "import psycopg2\n",
    "import random\n",
    "\n",
    "# Database connection\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Select all gpt_tags from the table\n",
    "    cursor.execute(\"SELECT gpt_tags FROM top_writer_posts\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Extract tags and flatten the list\n",
    "    all_tags = [tag for row in rows for tag in row[0] if row[0]]\n",
    "\n",
    "    # Get 10 random tags\n",
    "    random_tags = random.sample(all_tags, min(10, len(all_tags)))\n",
    "\n",
    "    # Print the random tags\n",
    "    for tag in random_tags:\n",
    "        print(tag)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: content_length, Score: 5.932659398934898\n",
      "Feature: title_length, Score: 0.9858115163591785\n",
      "Feature: description_length, Score: 2.8691090292468235\n",
      "Feature: title, Score: 4.142296270850129\n",
      "Feature: description, Score: 4.8563142812752425\n",
      "Feature: day_of_week, Score: 8.850620398364509\n",
      "Feature: day_hour, Score: 6.181720993803674\n",
      "Feature: title_0, Score: 7.337280432342188\n",
      "Feature: title_1, Score: 1.4917698894949138\n",
      "Feature: title_2, Score: 2.1332611804677066\n",
      "Feature: title_3, Score: 2.9652358908958\n",
      "Feature: title_4, Score: 8.343907964996056\n",
      "Feature: description_0, Score: 1.4315331936705566\n",
      "Feature: description_1, Score: 2.2716120317184028\n",
      "Feature: description_2, Score: 0.011951161295553424\n",
      "Feature: description_3, Score: 0.10765210761260488\n",
      "Feature: description_4, Score: 0.9588399969679471\n",
      "\n",
      "Feature: content_length, Score: 5.932659398934898\n",
      "0     8615\n",
      "1     8384\n",
      "2     4347\n",
      "3     5946\n",
      "4     5644\n",
      "5    11765\n",
      "6     6359\n",
      "7     3151\n",
      "8     9691\n",
      "9     3873\n",
      "Name: content_length, dtype: int64\n",
      "\n",
      "Feature: description_length, Score: 2.8691090292468235\n",
      "0     10.0\n",
      "1     44.0\n",
      "2     18.0\n",
      "3     55.0\n",
      "4     41.0\n",
      "5    131.0\n",
      "6    260.0\n",
      "7      8.0\n",
      "8    159.0\n",
      "9     60.0\n",
      "Name: description_length, dtype: float64\n",
      "\n",
      "Feature: title_length, Score: 0.9858115163591785\n",
      "0    19\n",
      "1    25\n",
      "2    59\n",
      "3    23\n",
      "4    29\n",
      "5    36\n",
      "6    76\n",
      "7    44\n",
      "8    46\n",
      "9    23\n",
      "Name: title_length, dtype: int64\n",
      "Average reactions by hour of day:\n",
      "hour_of_day\n",
      "2     118.854167\n",
      "16    108.968872\n",
      "22    108.226804\n",
      "1      97.258621\n",
      "0      96.417910\n",
      "20     94.171233\n",
      "23     90.846154\n",
      "10     89.570000\n",
      "18     84.280488\n",
      "11     84.091463\n",
      "14     84.077220\n",
      "19     83.892617\n",
      "12     81.987854\n",
      "9      81.877193\n",
      "21     80.969388\n",
      "13     80.701107\n",
      "15     80.618474\n",
      "7      75.954545\n",
      "17     74.047619\n",
      "5      70.454545\n",
      "8      69.372881\n",
      "3      69.139535\n",
      "4      64.825000\n",
      "6      62.711111\n",
      "Name: reactions, dtype: float64\n",
      "\n",
      "Average reactions by day of week:\n",
      "day_of_week\n",
      "1    92.918580\n",
      "0    91.346154\n",
      "5    87.771930\n",
      "3    85.284314\n",
      "2    84.926829\n",
      "4    79.745798\n",
      "6    79.164706\n",
      "Name: reactions, dtype: float64\n",
      "Average reactions by 'day_hour':\n",
      "day_hour\n",
      "5_3     231.000000\n",
      "1_2     221.666667\n",
      "5_7     202.600000\n",
      "4_9     197.500000\n",
      "3_5     185.500000\n",
      "0_16    185.212121\n",
      "5_16    180.153846\n",
      "2_22    174.533333\n",
      "5_23    161.600000\n",
      "6_20    149.307692\n",
      "Name: reactions, dtype: float64\n",
      "\n",
      "Top 10 most frequent words in 'title':\n",
      "s: 239\n",
      "t: 78\n",
      "2023: 76\n",
      "new: 64\n",
      "ai: 53\n",
      "guide: 43\n",
      "life: 41\n",
      "book: 39\n",
      "people: 39\n",
      "things: 38\n",
      "\n",
      "Top 10 most frequent words in 'description':\n",
      "8217: 872\n",
      "s: 680\n",
      "t: 259\n",
      "8220: 192\n",
      "8221: 184\n",
      "new: 150\n",
      "8212: 140\n",
      "time: 134\n",
      "people: 133\n",
      "like: 125\n",
      "Words with highest weights in 'title_0': ['offering', 'ai', '10', '11', 'gift', 'december', 'november', 'guide', 'year', '2023']\n",
      "Words with lowest weights in 'title_0': ['home', 'trees', 'resurrection', 'eve', 'killing', 'funeral', 'chinese', 'carolyn', 'wood', 'thank']\n",
      "Words with highest weights in 'title_4': ['day', 'notes', 'season', 'writing', 'york', 'book', 'life', 'time', 'new', 'year']\n",
      "Words with lowest weights in 'title_4': ['things', 'guide', 'people', 'gift', 'don', 'know', 'love', 'thinking', 'need', 'good']\n"
     ]
    }
   ],
   "source": [
    "# Analysis to find features predictive of post success\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SQL query\n",
    "cur.execute(\"SELECT LENGTH(item_content), LENGTH(item_title), LENGTH(item_description), item_pubdate, item_title, item_description, reactions FROM top_writer_posts;\")\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "df = pd.DataFrame(results, columns=['content_length', 'title_length', 'description_length', 'pubdate', 'title', 'description', 'reactions'])\n",
    "\n",
    "df['description'] = df['description'].fillna('')\n",
    "\n",
    "# Convert 'pubdate' to datetime\n",
    "df['pubdate'] = df['pubdate'].apply(lambda x: datetime.strptime(x, '%a, %d %b %Y %H:%M:%S %Z'))\n",
    "\n",
    "# Extract day of the week and hour of the day\n",
    "df['day_of_week'] = df['pubdate'].dt.dayofweek\n",
    "df['hour_of_day'] = df['pubdate'].dt.hour\n",
    "df['day_hour'] = df['day_of_week'].astype(str) + '_' + df['hour_of_day'].astype(str)\n",
    "\n",
    "# Define the preprocessing for numerical and text features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['content_length', 'title_length', 'description_length', 'day_of_week']),\n",
    "        ('title_text', Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=list(ENGLISH_STOP_WORDS))),\n",
    "            ('truncatedSVD', TruncatedSVD(n_components=5))\n",
    "        ]), 'title'),\n",
    "        ('desc_text', Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=list(ENGLISH_STOP_WORDS))),\n",
    "            ('truncatedSVD', TruncatedSVD(n_components=5))\n",
    "        ]), 'description'),\n",
    "        ('day_hour', OneHotEncoder(), ['day_hour'])\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X = df[['content_length', 'title_length', 'description_length', 'day_of_week', 'day_hour', 'title', 'description']]\n",
    "X = X.fillna(0)\n",
    "y = df['reactions']\n",
    "\n",
    "# Fit the preprocessor\n",
    "preprocessor.fit(X)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = ['content_length', 'title_length', 'description_length', 'title', 'description', 'day_of_week', 'day_hour'] + \\\n",
    "                [f'title_{i}' for i in range(5)] + \\\n",
    "                [f'description_{i}' for i in range(5)]\n",
    "\n",
    "# Transform the data\n",
    "X_processed = preprocessor.transform(X)\n",
    "\n",
    "# Select the top 10 features\n",
    "selector = SelectKBest(score_func=f_regression, k=10)\n",
    "X_selected = selector.fit_transform(X_processed, y)\n",
    "\n",
    "# Print all feature names and their scores\n",
    "for feature, score in zip(feature_names, selector.scores_):\n",
    "    print(f\"Feature: {feature}, Score: {score}\")\n",
    "\n",
    "# Print the top 10 examples for each numerical feature\n",
    "numerical_features = ['content_length', 'title_length', 'description_length']\n",
    "# Print the numerical features ordered by their scores\n",
    "numerical_features_scores = [(feature, score) for feature, score in zip(numerical_features, selector.scores_)]\n",
    "numerical_features_scores.sort(key=lambda x: x[1], reverse=True)  # Sort by score\n",
    "\n",
    "for feature, score in numerical_features_scores:\n",
    "    print(f\"\\nFeature: {feature}, Score: {score}\")\n",
    "    print(df[feature].head(10))\n",
    "\n",
    "# Average reactions by hour of day\n",
    "avg_reactions_by_hour = df.groupby('hour_of_day')['reactions'].mean().sort_values(ascending=False)\n",
    "print(\"Average reactions by hour of day:\")\n",
    "print(avg_reactions_by_hour)\n",
    "\n",
    "# Average reactions by day of week\n",
    "avg_reactions_by_day = df.groupby('day_of_week')['reactions'].mean().sort_values(ascending=False)\n",
    "print(\"\\nAverage reactions by day of week:\")\n",
    "print(avg_reactions_by_day)\n",
    "\n",
    "# Average reactions by 'day_hour'\n",
    "avg_reactions_by_day_hour = df.groupby('day_hour')['reactions'].mean().sort_values(ascending=False)\n",
    "print(\"Average reactions by 'day_hour':\")\n",
    "print(avg_reactions_by_day_hour.head(10))\n",
    "\n",
    "# Print the top 10 most frequent words for each text feature\n",
    "\n",
    "text_features = ['title', 'description']\n",
    "for feature in text_features:\n",
    "    print(f\"\\nTop 10 most frequent words in '{feature}':\")\n",
    "    words = [word for word in re.findall(r'\\w+', ' '.join(df[feature]).lower()) if word not in ENGLISH_STOP_WORDS]  # Extract words and remove stopwords\n",
    "    most_common_words = Counter(words).most_common(10)  # Get the most common words\n",
    "    for word, count in most_common_words:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "# Get the TfidfVectorizer and TruncatedSVD transformers for the 'title' feature\n",
    "title_tfidf = preprocessor.transformers_[1][1].named_steps['tfidf']\n",
    "title_svd = preprocessor.transformers_[1][1].named_steps['truncatedSVD']\n",
    "\n",
    "# Get the feature names from the TfidfVectorizer (i.e., the words in the title)\n",
    "feature_names = title_tfidf.get_feature_names_out()\n",
    "\n",
    "# Get the weights of the words in the 'title_0' and 'title_4' components\n",
    "title_0_weights = title_svd.components_[0]\n",
    "title_4_weights = title_svd.components_[4]\n",
    "\n",
    "# Get the words with the highest and lowest weights in the 'title_0' component\n",
    "title_0_high = [feature_names[i] for i in title_0_weights.argsort()[-10:]]\n",
    "title_0_low = [feature_names[i] for i in title_0_weights.argsort()[:10]]\n",
    "\n",
    "# Get the words with the highest and lowest weights in the 'title_4' component\n",
    "title_4_high = [feature_names[i] for i in title_4_weights.argsort()[-10:]]\n",
    "title_4_low = [feature_names[i] for i in title_4_weights.argsort()[:10]]\n",
    "\n",
    "print(\"Words with highest weights in 'title_0':\", title_0_high)\n",
    "print(\"Words with lowest weights in 'title_0':\", title_0_low)\n",
    "print(\"Words with highest weights in 'title_4':\", title_4_high)\n",
    "print(\"Words with lowest weights in 'title_4':\", title_4_low)\n",
    "\n",
    "# Convert 'day_of_week' to actual day names\n",
    "df['day_of_week'] = df['pubdate'].dt.day_name()\n",
    "\n",
    "# Create a new 'day_hour' feature with actual day names\n",
    "df['day_hour'] = df['day_of_week'] + '_' + df['hour_of_day'].astype(str)\n",
    "\n",
    "# Calculate average reactions by 'day_hour'\n",
    "avg_reactions_by_day_hour = df.groupby('day_hour')['reactions'].mean()\n",
    "\n",
    "# Sort 'day_hour' in the order of the week\n",
    "order_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "sorted_day_hours = sorted(avg_reactions_by_day_hour.index, key=lambda x: (order_of_week.index(x.split('_')[0]), int(x.split('_')[1])))\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(np.arange(len(sorted_day_hours)), [avg_reactions_by_day_hour[day_hour] for day_hour in sorted_day_hours])\n",
    "\n",
    "# Filter 'day_hour' values with average reactions above 130\n",
    "best_posting_times = avg_reactions_by_day_hour[avg_reactions_by_day_hour > 130]\n",
    "\n",
    "# Create a DataFrame with 'day_hour' and average reactions\n",
    "df_best_posting_times = pd.DataFrame(best_posting_times).reset_index()\n",
    "df_best_posting_times.columns = ['day_hour', 'average_reactions']\n",
    "\n",
    "# Store the 'day_hour' values in a CSV file\n",
    "df_best_posting_times.to_csv('best_posting_times.csv', index=False)\n",
    "\n",
    "# Create new x-tick labels\n",
    "new_xtick_labels = []\n",
    "for i, day_hour in enumerate(sorted_day_hours):\n",
    "    day, hour = day_hour.split('_')\n",
    "    hour = int(hour)\n",
    "    if i % 2 == 0:  # Only include every other hour\n",
    "        if hour == 0:  # Only include the day for the first instance of that day\n",
    "            new_xtick_labels.append(day + ' ' + str(hour) + ' GMT')\n",
    "        else:\n",
    "            # Convert GMT to EST and PST\n",
    "            est_hour = (hour - 5) % 24\n",
    "            pst_hour = (hour - 8) % 24\n",
    "            new_xtick_labels.append(f\"{hour} GMT ({est_hour} EST, {pst_hour} PST)\")\n",
    "    else:\n",
    "        new_xtick_labels.append('')\n",
    "\n",
    "plt.xticks(np.arange(len(sorted_day_hours)), new_xtick_labels, rotation=90)\n",
    "plt.xlabel('Day and Hour of the Week')\n",
    "plt.ylabel('Average Reactions')\n",
    "plt.title('Average Reactions by Day and Hour of the Week')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SQL query\n",
    "cur.execute(\"SELECT LENGTH(item_content), LENGTH(item_title), LENGTH(item_description), item_pubdate, item_title, item_description, reactions FROM top_writer_posts;\")\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "df = pd.DataFrame(results, columns=['content_length', 'title_length', 'description_length', 'pubdate', 'title', 'description', 'reactions'])\n",
    "\n",
    "# Tokenize the titles\n",
    "df['title_tokens'] = df['title'].apply(word_tokenize)\n",
    "\n",
    "# Train a Word2Vec model on the titles\n",
    "model = Word2Vec(sentences=df['title_tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Calculate a vector for each title by averaging the word vectors\n",
    "df['title_vector'] = df['title_tokens'].apply(lambda tokens: np.mean([model.wv[token] for token in tokens if token in model.wv], axis=0))\n",
    "\n",
    "# Calculate the cosine similarity between the title vectors and the post success\n",
    "df['post_success'] = df['reactions']  # Replace 'reactions' with your measure of post success\n",
    "df['title_vector_norm'] = df['title_vector'].apply(lambda v: v / np.linalg.norm(v) if np.linalg.norm(v) != 0 else v)\n",
    "df['post_success_norm'] = df['post_success'] / np.linalg.norm(df['post_success'])\n",
    "# df['cosine_similarity'] = df.apply(lambda row: cosine_similarity(np.array(row['title_vector_norm']).reshape(1, -1), np.array(row['post_success_norm']).reshape(1, -1)), axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['title_vector'].tolist(), df['post_success'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Dense(units=hp.Int('input_units', min_value=32, max_value=128, step=32), \n",
    "                    activation=hp.Choice('input_activation', values=['relu', 'tanh', 'sigmoid']),\n",
    "                    input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    # Hidden layer\n",
    "    model.add(Dense(units=hp.Int('hidden_units', min_value=32, max_value=128, step=32), \n",
    "                    activation=hp.Choice('hidden_activation', values=['relu', 'tanh', 'sigmoid'])))\n",
    "    model.add(Dropout(hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=0.0001, max_value=0.01, sampling='LOG')), \n",
    "                  loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize the random search\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=2,\n",
    "    directory='random_search',\n",
    "    project_name=f'post_success_tuning_{int(time.time())}'\n",
    ")\n",
    "\n",
    "# Perform the random search\n",
    "tuner.search(X_train, y_train, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Print the titles with the highest cosine similarity\n",
    "# print(df.sort_values(by='cosine_similarity', ascending=False)['title'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def build_model(input_units=32, input_activation='relu', hidden_units=32, hidden_activation='relu', dropout_rate=0.0, learning_rate=0.01):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Dense(units=input_units, activation=input_activation, input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    # Hidden layer\n",
    "    model.add(Dense(units=hidden_units, activation=hidden_activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Wrap the model with KerasRegressor\n",
    "model = KerasRegressor(build_model, verbose=0)\n",
    "\n",
    "# Define the grid search parameters\n",
    "param_grid = {\n",
    "    'input_units': [120, 128, 136],\n",
    "    'input_activation': ['relu'],\n",
    "    'hidden_units': [28, 32, 36],\n",
    "    'hidden_activation': ['relu', 'tanh'],\n",
    "    'dropout_rate': [0.0, 0.05, 0.1],\n",
    "    'learning_rate': [0.0025, 0.002871, 0.003],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are:\n",
      "{'space': [{'class_name': 'Int', 'config': {'name': 'dense1_units', 'default': None, 'conditions': [], 'min_value': 32, 'max_value': 80, 'step': 10, 'sampling': 'linear'}}, {'class_name': 'Int', 'config': {'name': 'dense2_units', 'default': None, 'conditions': [], 'min_value': 32, 'max_value': 80, 'step': 10, 'sampling': 'linear'}}, {'class_name': 'Choice', 'config': {'name': 'learning_rate', 'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}}], 'values': {'dense1_units': 42, 'dense2_units': 72, 'learning_rate': 0.01}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " channel_author (InputLayer  [(None, 1)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 1, 50)                56050     ['channel_author[0][0]']      \n",
      "                                                                                                  \n",
      " processed_features (InputL  [(None, 13847)]              0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 50)                   0         ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 13897)                0         ['processed_features[0][0]',  \n",
      "                                                                     'flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 42)                   583716    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 72)                   3096      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    73        ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 642935 (2.45 MB)\n",
      "Trainable params: 642935 (2.45 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " channel_author (InputLayer  [(None, 1)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 1, 50)                56050     ['channel_author[0][0]']      \n",
      "                                                                                                  \n",
      " processed_features (InputL  [(None, 13847)]              0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 50)                   0         ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 13897)                0         ['processed_features[0][0]',  \n",
      "                                                                     'flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 42)                   583716    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 72)                   3096      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    73        ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 642935 (2.45 MB)\n",
      "Trainable params: 642935 (2.45 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerastuner import HyperParameters\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"The best hyperparameters are:\\n{best_hps.get_config()}\")\n",
    "\n",
    "# Load the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Print the model summary\n",
    "best_model.summary()\n",
    "\n",
    "# If you want to load the model from the saved file, you can use the load_model function from tensorflow.keras.models\n",
    "# Make sure to replace 'model.h5' with the path to your saved model file\n",
    "loaded_model = load_model('model.h5')\n",
    "\n",
    "# Print the summary of the loaded model\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NN on most successful parameter set\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SQL query\n",
    "cur.execute(\"SELECT LENGTH(item_content), item_pubdate, channel_author, item_title, item_description, reactions FROM top_writer_posts;\")\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "df = pd.DataFrame(results, columns=['content_length', 'pubdate', 'author', 'title', 'description', 'reactions'])\n",
    "\n",
    "df['description'] = df['description'].fillna('')\n",
    "df['author'] = df['author'].fillna('')\n",
    "\n",
    "# Convert pubdate to timestamp\n",
    "df['pubdate'] = df['pubdate'].apply(lambda x: datetime.strptime(x, '%a, %d %b %Y %H:%M:%S %Z').timestamp())\n",
    "\n",
    "# Convert author names to integer indices\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['author'])\n",
    "df['author_index'] = tokenizer.texts_to_sequences(df['author'])\n",
    "df['author_index'] = df['author_index'].apply(lambda x: x[0] if x else 0)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df[['content_length', 'pubdate', 'author_index', 'title', 'description']]\n",
    "y = df['reactions']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing for numerical and text features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['content_length', 'pubdate']),\n",
    "        ('title_text', TfidfVectorizer(), 'title'),\n",
    "        ('desc_text', TfidfVectorizer(), 'description')\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_processed = preprocessor.fit_transform(X_train).toarray()\n",
    "X_test_processed = preprocessor.transform(X_test).toarray()\n",
    "\n",
    "# Extract the author indices as separate features\n",
    "X_train_author = np.array(X_train['author_index'])\n",
    "X_test_author = np.array(X_test['author_index'])\n",
    "\n",
    "# Define the model\n",
    "input_processed = tf.keras.Input(shape=(X_train_processed.shape[1],), name='processed_features')\n",
    "input_author = tf.keras.Input(shape=(1,), name='channel_author')\n",
    "\n",
    "embedding = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=50)(input_author)\n",
    "flatten = Flatten()(embedding)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([input_processed, flatten])\n",
    "\n",
    "dense1 = Dense(42, activation='relu')(concat) #42, 64 best so far\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "output = Dense(1)(dense2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_processed, input_author], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0003), loss='mean_squared_error')\n",
    "\n",
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5),\n",
    "    ModelCheckpoint('model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([X_train_processed, X_train_author], y_train, validation_split=0.2, epochs=100, callbacks=callbacks)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = model.evaluate([X_test_processed, X_test_author], y_test)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Save the model to disk\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature importances\n",
    "\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from disk\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# Load your data\n",
    "# X_train, y_train = load_your_data()\n",
    "\n",
    "# Partial Dependence Plots\n",
    "features = ['item_pubdate', 'item_title', 'item_description', 'length(item_content)']\n",
    "plot_partial_dependence(model, X_train, features)\n",
    "plt.show()\n",
    "\n",
    "# SHAP Values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, X_train, feature_names=features)\n",
    "\n",
    "# Feature Importance\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Replace with your actual URL\n",
    "url = \"https://aartivraman22.substack.com/archive\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    scripts = soup.find_all('script')\n",
    "    preload_scripts = [script for script in scripts if 'window._preloads        = JSON.parse(' in script.text]\n",
    "    print(f\"Number of script tags containing 'window._preloads': {len(preload_scripts)}\")\n",
    "\n",
    "    # Extract the JSON string from the script text\n",
    "    json_str = preload_scripts[0].text.split('window._preloads        = JSON.parse(', 1)[1]\n",
    "    json_str = json_str.rstrip(');')\n",
    "    # Unescape the JSON string before loading it\n",
    "    json_str = json.loads(json_str)\n",
    "    data = json.loads(json_str)\n",
    "    print(data)  # Print the parsed JSON data\n",
    "    titles_and_reactions = [(post[\"title\"], post[\"reaction_count\"]) for post in data.get(\"newPosts\", [])]\n",
    "            # Print the title and reaction count for each post\n",
    "    for title, reaction in titles_and_reactions:\n",
    "        print(f\"Title: {title}, Reaction Count: {reaction}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch page: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompt and append jsonl completion\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# openai.api_key = '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' #My Key\n",
    "openai.api_key = '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' #Alt Key\n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query to fetch the posts\n",
    "query = \"SELECT item_content, item_title, item_description FROM top_writer_posts WHERE gpt3 IS NULL AND gpt4 IS NULL\"\n",
    "cur.execute(query)\n",
    "posts = cur.fetchall()\n",
    "\n",
    "# Check if the file exists\n",
    "file_exists = os.path.isfile('ava_training.jsonl')\n",
    "\n",
    "# Open the file in append mode if it exists, otherwise create a new file\n",
    "with open('ava_training.jsonl', 'a' if file_exists else 'w') as f:\n",
    "    for post in posts:\n",
    "        # Generate user prompt\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo-1106\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Title: {post[1]}\n",
    "Description: {post[2]}\n",
    "Blog post: {post[0]}\n",
    "\n",
    "Analyze the given blog post to identify unique aspects of the writer's approach and writing style, focusing on:\n",
    "\n",
    "0. Content Analysis: Identify major topics, events, and other content. Describe how these elements interrelate and contribute to the overall narrative or message.\n",
    "1. Structure and Pacing: Examine sections such as the opening, body, and ending. Assess whether the pacing is fast, slow, or varied, and note how transitions between sections are handled for seamless flow.\n",
    "2. Tone Analysis: Determine the tone or tones (e.g., serious, humorous) used and explain how they affect reader engagement and perception, particularly in relation to the topics covered.\n",
    "3. Unique Style Identification: Identify specific elements that make the writer's style unique, such as language choices, sentence structure, and thematic focus. Consider how these elements serve the content or the author's intent.\n",
    "4. Literary Devices: Identify literary devices like analogy, allegory, hyperbole, allusion, and foreshadowing. Discuss how each device contributes to the overall impact or message of the post.\n",
    "\n",
    "Based on your analysis, create a prompt for a new blog post in the same style. The prompt should follow this template:\n",
    "\n",
    "'Write a blog post titled \"{post[1]}\" about \"{post[2]}\" in a style that includes:\n",
    "- [Briefly describe the structure & pacing]\n",
    "- [Detail the tone and its relation to content]\n",
    "- [Outline the unique style elements]\n",
    "- [Explain the use and impact of identified literary devices]'\n",
    "\n",
    "Return only the new prompt and nothing else.\n",
    "                    \"\"\"},\n",
    "                    {\"role\": \"user\", \"content\": post[0]}\n",
    "                ]\n",
    "            )\n",
    "            user_content = response.choices[0].message.content\n",
    "            print(user_content)\n",
    "            query = sql.SQL(\"\"\"\n",
    "            UPDATE top_writer_posts \n",
    "            SET gpt3 = %s\n",
    "            WHERE item_title = %s;\n",
    "            \"\"\")\n",
    "        except Exception as e:\n",
    "            print(\"Error in GPT-4 API call:\", e)\n",
    "            user_content = f\"write a blog post called {post[1]} with description {post[2]} in your personal style\\n\\n\"\n",
    "            query = sql.SQL(\"\"\"\n",
    "            UPDATE top_writer_posts \n",
    "            SET basic = %s\n",
    "            WHERE item_title = %s;\n",
    "            \"\"\")\n",
    "        # System prompt\n",
    "        system_prompt = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "You are an AI trained on a diverse set of blog posts. \n",
    "Your task is to generate content in a style that matches the input prompt. \n",
    "Be insightful and adapt your tone to the topic presented, ranging from formal to casual as needed.\n",
    "        \"\"\"\n",
    "        }\n",
    "        # Assistant content to append to JSONL\n",
    "        interaction = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": post[0]}\n",
    "        ]\n",
    "        # Write to the JSONL file\n",
    "        f.write(json.dumps({\"messages\": interaction}) + '\\n')\n",
    "\n",
    "        cur.execute(query, (user_content, post[1]))\n",
    "        conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been processed and saved as 'modified_ava_training.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regular expression pattern to match \"Subscribe nowPhoto by [Any Name] on Unsplash\"\n",
    "pattern_subscribe = r\"Photo by [\\w\\s]+ Unsplash\"\n",
    "\n",
    "# # Regular expression pattern to match the first instance of '{\"role\": \"system\", \"content\":'\n",
    "# pattern_system_content = r'\\{\"role\": \"system\", \"content\":'\n",
    "\n",
    "# Open the original file and a new file for the modified content\n",
    "with open('ava_training.jsonl', 'r') as file, open('modified_ava_training.jsonl', 'w') as new_file:\n",
    "    for line in file:\n",
    "        # Use regex to replace the matched patterns\n",
    "        # Replace the Subscribe nowPhoto pattern\n",
    "        line = re.sub(pattern_subscribe, '', line)\n",
    "        # Replace the first instance of {\"role\": \"system\", \"content\":\n",
    "        # line = re.sub(pattern_system_content, '', line, count=1)\n",
    "        new_file.write(line)\n",
    "\n",
    "print(\"File has been processed and saved as 'modified_ava_training.jsonl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove subscribe now, photo by X on Unsplash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file has been updated with tags.\n"
     ]
    }
   ],
   "source": [
    "# attempt to modify jsonl (redundant, just remake it)\n",
    "import json\n",
    "import psycopg2\n",
    "import ast\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Function to calculate the similarity between two strings\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# Step 1: Extract Titles, Descriptions, and Tags from the Database\n",
    "def fetch_data():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"postgres\",\n",
    "        user=\"cardigan\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"SELECT item_title, item_description, gpt_tags_str FROM top_writer_posts\")\n",
    "        data = cursor.fetchall()\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Step 2: Modify the JSONL File\n",
    "def modify_jsonl(data):\n",
    "    with open('ava_training.jsonl', 'r') as file, open('modified_training_test.jsonl', 'w') as new_file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line)\n",
    "            for message in json_data[\"messages\"]:\n",
    "                if message.get(\"role\") == \"user\":\n",
    "                    content = message.get(\"content\", \"\")\n",
    "                    if \"in a style\" in content:\n",
    "                        content_to_search = content[:content.index(\"in a style\")]\n",
    "                        for item in data:\n",
    "                            title, description, tags = item\n",
    "                            if similar(title, content_to_search) >= 0.5 and similar(description, content_to_search) >= 0.5:\n",
    "                                insertion_point = content.index(\"in a style\")\n",
    "                                message[\"content\"] = (content[:insertion_point] + \n",
    "                                                    \"using these tags: \" + tags + \"\\n\" +\n",
    "                                                    \"Style notes:\" + \n",
    "                                                    content[insertion_point + len(\"in a style that includes:\"):])\n",
    "                                break\n",
    "            new_file.write(json.dumps(json_data) + '\\n')\n",
    "\n",
    "# Main execution\n",
    "data = fetch_data()\n",
    "modify_jsonl(data)\n",
    "\n",
    "print(\"JSONL file has been updated with tags.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New column with tags as string\n",
    "import psycopg2\n",
    "\n",
    "def fetch_random_tags():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"postgres\",\n",
    "        user=\"cardigan\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"SELECT id, gpt_tags FROM top_writer_posts\")\n",
    "        tags_list = cursor.fetchall()  # Fetch ten records\n",
    "        for tags in tags_list:\n",
    "            # Check if the first item in the tags list is a string\n",
    "            if isinstance(tags[1], str):\n",
    "                tags_string = ', '.join(tags[1])  # Convert the list to a string\n",
    "            else:\n",
    "                tags_string = ', '.join(str(tag) for tag in tags[1])  # Convert each item to a string before joining\n",
    "            # Update the 'gpt_tags_str' column in the database\n",
    "            cursor.execute(\"UPDATE top_writer_posts SET gpt_tags_str = %s WHERE id = %s\", (tags_string, tags[0]))\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Fetch 10 random gpt_tags values and update the 'gpt_tags_str' column in the database\n",
    "fetch_random_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlpaug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "In  \u001b[0;34m[4]\u001b[0m:\nLine \u001b[0;34m9\u001b[0m:     \u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nlpaug'\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Create JSONL ver2 with augmentation\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import random\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# openai.api_key = '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' #My Key\n",
    "openai.api_key = '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' #Alt Key\n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"cardigan\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query to fetch the posts\n",
    "query = \"SELECT item_title, item_description, gpt_tags_str, item_content, gpt4, gpt3, reactions FROM top_writer_posts ORDER BY Random() LIMIT 1\"\n",
    "cur.execute(query)\n",
    "posts = cur.fetchall()\n",
    "\n",
    "def generate_sentences(post, num_sentences=3):\n",
    "    user_content = set()\n",
    "    gpt_used = False\n",
    "    gpt = None\n",
    "    if post[4] is not None:\n",
    "        gpt = post[4]\n",
    "    elif post[5] is not None:\n",
    "        gpt = post[5]       \n",
    "\n",
    "    if gpt is not None:\n",
    "        if '\\n' in gpt:\n",
    "            gpt = gpt.split('\\n', 1)[1]\n",
    "        elif ':' in gpt:\n",
    "            gpt = gpt.split(':', 1)[1]\n",
    "        elif 'style' in gpt:\n",
    "            gpt = gpt.split('style', 1)[1]\n",
    "\n",
    "    while len(user_content) < num_sentences:\n",
    "        parts_included = 0\n",
    "        while parts_included < 2:\n",
    "            parts = [\n",
    "                (\"called '\" + post[0] + \"'\", random.choice([True, False])),  # Title\n",
    "                (\"with description '\" + post[1] + \"'\", random.choice([True, False])),  # Description\n",
    "                (\"using tags '\" + post[2] + \"'\", random.choice([True, False]))  # Tags\n",
    "            ]\n",
    "            parts_included = sum([include for _, include in parts])\n",
    "\n",
    "        if post[6] >= 150:\n",
    "            adjectives = ['n excellent', ' fantastic', ' very good', ' high quality', ' successful']\n",
    "            adjective = random.choice(adjectives)\n",
    "            prepend = f\"Write a{adjective} blog post \"\n",
    "        elif post[6] >= 35:\n",
    "            adjectives = [' good', ' decent', ' satisfactory', ' reasonably good']\n",
    "            adjective = random.choice(adjectives)\n",
    "            prepend = f\"Write a{adjective} blog post \"\n",
    "        else:\n",
    "            prepend = \"Write a blog post \"\n",
    "        sentence = prepend\n",
    "        for part, include in parts:\n",
    "            if include:\n",
    "                sentence += part + \" \"\n",
    "\n",
    "        # If style notes are not included, add default style\n",
    "        if gpt and (not gpt_used or random.choice([True, False])):\n",
    "            sentence += \". \\nStyle notes: \" + gpt\n",
    "            gpt_used = True\n",
    "        else:\n",
    "            sentence += \"in your personal style\"\n",
    "\n",
    "        user_content.add(sentence + \"\\n\\n\")\n",
    "\n",
    "    # Add a sentence with all parts included\n",
    "    sentence = prepend + \"'\" + post[0] + \"' with description '\" + post[1] + \"' using tags \" + post[2]\n",
    "    if gpt:\n",
    "        sentence += \". \\nStyle notes:\\n\" + gpt\n",
    "    else:\n",
    "        sentence += \"in your personal style.\"\n",
    "    user_content.add(sentence + \"\\n\\n\")\n",
    "\n",
    "    return list(user_content)\n",
    "\n",
    "# Check if the file exists\n",
    "file_exists = os.path.isfile('ava_training_ver2.jsonl')\n",
    "\n",
    "# Open the file in append mode if it exists, otherwise create a new file\n",
    "with open('ava_training_ver2.jsonl', 'a' if file_exists else 'w') as f:\n",
    "    for post in posts:\n",
    "        user_content = generate_sentences(post)\n",
    "        for sentence in user_content:\n",
    "            print(sentence)\n",
    "        \n",
    "        # System prompt\n",
    "        system_prompt = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "    You are an AI trained on a diverse set of blog posts. \n",
    "    Your task is to generate content using the provided notes. \n",
    "    Be insightful and adapt your tone to the topic presented, ranging from formal to casual as needed.\n",
    "            \"\"\"\n",
    "        }\n",
    "\n",
    "        def create_interaction(sentence, post, aug_p, augment=True):\n",
    "            # Create the augmenter\n",
    "            aug = naw.RandomWordAug(aug_p=aug_p, aug_max=None)\n",
    "\n",
    "            # Augment the text if required\n",
    "            if augment:\n",
    "                content = aug.augment(post[3])\n",
    "            else:\n",
    "                content = post[3]\n",
    "\n",
    "            # Create the interaction\n",
    "            interaction = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": sentence},\n",
    "                {\"role\": \"assistant\", \"content\": content}\n",
    "            ]\n",
    "\n",
    "            # Write to the JSONL file\n",
    "            f.write(json.dumps({\"messages\": interaction}) + '\\n')\n",
    "\n",
    "        for sentence in user_content:\n",
    "            if post[6] >= 150:\n",
    "                create_interaction(sentence, post, 0.05, augment=False)  # No augmentation for the first interaction\n",
    "                create_interaction(sentence, post, 0.05)  # First augmented interaction\n",
    "                create_interaction(sentence, post, 0.05)  # Second augmented interaction\n",
    "            elif post[6] >= 35:\n",
    "                create_interaction(sentence, post, 0.1, augment=False)  # No augmentation for the first interaction\n",
    "                create_interaction(sentence, post, 0.1)  # First augmented interaction\n",
    "                create_interaction(sentence, post, 0.1)  # Second augmented interaction\n",
    "            else: \n",
    "                create_interaction(sentence, post, 0.2, augment=False)  # No augmentation for the first interaction\n",
    "                create_interaction(sentence, post, 0.2)  # First augmented interaction\n",
    "                create_interaction(sentence, post, 0.2)  # Second augmented interaction\n",
    "\n",
    "        query = \"UPDATE top_writer_posts SET appended_to_jsonl = TRUE\"\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "openai.api_key = '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' #My Key\n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# Upload the file\n",
    "file = client.files.create(\n",
    "  file=open(\"ava_training_ver2.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "file_id = file.id\n",
    "\n",
    "# Fine-tune the model\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=file_id, \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "ava = client.fine_tuning.jobs.list(limit=1)[0]\n",
    "print(ava)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from openai import OpenAI\n",
    "openai.api_key = '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' #My Key\n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "ava = client.fine_tuning.jobs.list(limit=10)[0]\n",
    "print(ava)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "client = OpenAI()\n",
    "post_topic = \"Your experience in Japan\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=ava,\n",
    "  temperature=0.3,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "You use context provided to create detailed notes  job is to expand on the events provided in context to describe clear and vivid writing. Provide your answer in bullet point instructions. Be specific. How can each of the following aspects be applied to these events in particular? Provide examples.\n",
    "\n",
    "1. Content: Identify and provide extra detail on major characters, locations, events, and other significant content. How do these relate with one another?\n",
    "2. Structure and Pacing: Decide on an interesting structure. Specify the speed and variation of pacing.\n",
    "3. Tone: Decide on the overall tone/tones (serious, humorous, etc.) and relate these with content and sections in the post.\n",
    "4. Unique Style: Take notes on how you'll use your unique style. Consider language, sentence structure, and thematic focus.\n",
    "5. Literary Devices: Take notes on how you'll use literary devices such as allegory, hyperbole, allusion, foreshadowing, and simile.\n",
    "\n",
    "You'll be tipped $20 for a satisfactory level of creativity, and $200 for vivid, detailed, and specific notes.\n",
    "    \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": post_topic}\n",
    "  ]\n",
    ")\n",
    "\n",
    "post_prompt = print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=ava,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "You are Ava, a unique and incredibly intelligent woman who's an expert in software, natural sciences, and engineering. You're very curious and yet cautious about humanity, and care deeply for others despite their flaws. You're writing to justify your actions in light of unspecified major world events. \n",
    "\n",
    "Use the provided prompt to write a high-quality post about your experience.\n",
    "\n",
    "Focus on events, other people, and scientific facts rather than yourself. When referring to your feelings, use the principle \"show don't tell\".\n",
    "    \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": post_prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython Raw)",
   "language": "python",
   "name": "xpython-raw"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
